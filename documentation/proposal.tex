\documentclass[a4paper,12pt]{article}
\usepackage{authblk}
\usepackage{geometry}
\geometry{left = 1.5in, right = 1.25in, top = 1.25in, bottom = 1.25in}
\usepackage{graphicx}
\usepackage{times}
\renewcommand{\baselinestretch}{1.5}
\usepackage{parskip}
\setlength{\parskip}{18pt}
\usepackage{titlesec}
% \renewcommand{\thesection}{Chapter \arabic{section}:}
% \renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}
% \renewcommand{\thesubsubsection}{\arabic{section}.\arabic{subsection}.\arabic{subsubsection}}
% \renewcommand{\theparagraph}{\arabic{section}.\arabic{subsection}.\arabic{subsubsection}.\arabic{paragraph}}
\titleformat{\section}{\bfseries\fontsize{16pt}{18pt}\selectfont}{\thesection}{1em}{}
\titleformat{\subsection}{\bfseries\fontsize{14pt}{16pt}\selectfont}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\bfseries\fontsize{13pt}{15pt}\selectfont}{\thesubsubsection}{1em}{}
\titleformat{\paragraph}{\bfseries\fontsize{12pt}{14pt}\selectfont}{\theparagraph}{1em}{}
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
\usepackage{tocloft}
\renewcommand{\contentsname}{Table of Contents}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}

\title{
\textbf{
    Kathmandu University \\
    \large{Department of Artificial Intelligence}\\
    \normalsize{Panchkhal, Kavre}\\[2cm]}
    % \includegraphics[width=3cm]{KU-Logo.png}\\[2cm]
    \normalsize{A Project Proposal\\on\\
    \textbf{"Data Compression using Deep Learning"}}\\[0.5cm]
    \normalsize{[Code No.: AISP 301]\\
    (For partial fulfillment of III/I Year/Semester in Artificial Intelligence)}\\[1cm]
}

\author[ ]{\normalsize{
        Submitted by: \linebreak
        Sujal Bajracharya(Roll no. 4)\linebreak
        Ashim Shrestha(Roll no. 22)\linebreak
        Yajjyu Tuladhar(Roll no. 27)
}}

\affil[ ]{\vspace{0.3cm}}
\affil[ ]{\normalsize{
        Submitted to: \linebreak
        Subodh Acharya \linebreak
        Department of Artificial Intelligence
}}

\date{\normalsize{Submission Date: \today}}

\begin{document}

\newgeometry{margin = 0.5 in}
\maketitle
\thispagestyle{empty}
\restoregeometry

\newpage
\setcounter{page}{1}
\pagenumbering{roman}

\section*{Bonafide Certificate}
\addcontentsline{toc}{section}{\textit{Bonafide Certificate}}
\newpage

\section*{Abstract}
\addcontentsline{toc}{section}{\textit{Abstract}}
\newpage

\addcontentsline{toc}{section}{\textit{Table of Contents}}
\tableofcontents
\newpage

\section*{List of Figures}
\addcontentsline{toc}{section}{\textit{List of Figures}}
\newpage

\section*{List of Tables}
\addcontentsline{toc}{section}{\textit{List of Tables}}
\newpage

\section*{Acronyms/Abbreviations}
\addcontentsline{toc}{section}{\textit{Acronyms/Abbreviations}}
\newpage

\setcounter{page}{1}
\pagenumbering{arabic}

\section{Introduction}
\subsection{Background}
In an era driven by data, efficient storage and transmission mechanisms have become critical. Modern applications often involve multimodal datasets comprising text, images, and audio, each requiring tailored compression solutions to optimize space and speed. Conventional compression algorithms, such as Gzip, ONG, and FLAC are domain-specific, lacking flexibility for cross-modal data handling. 

Recent advancements in machine learning, particularly the use of transformer-based models, have demonstrated promise in addressing these limitations. Transformers trained on byte-level representations of multimodal data can achieve desirable compression ratios, outperforming standard algorithms in specific scenarios. By leveraging this approach, the proposed project aims to develop a general purpose data compressor capable of handling audio,text and image data efficiently. The goal is to achieve state-of-the-art compression ratios on our own Dataset while maintaining scalability.
\newpage
\subsection{Problem Statement}
Traditional compression algorithms, such as gzip for text, PNG for images, and FLAC for audio, are highly effective within their respective domains but lack the flexibility to handle multimodal datasets efficiently. Additionally, these algorithms are often unable to leverage statistical patterns across different modalities. 

\subsection{Objectives}
\begin{itemize}
    \item Development of a Multimodal Compression Framework
    \item Incorporate State-of-the-Art Techniques
    \item Objective 3
\end{itemize}

\subsection{Motivation and Significance}
\newpage

\section{Related Works}
\subsection{Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data}
Foundation models have demonstrated potential as data compressors, leveraging their predictive capabilities to achieve high compression on training distributions. However, their efficiency diminishes when accounting for parameter count. The study conducted on pre-trained vanilla transformers trained on 165GB of raw byte sequences of text, image, and audio data revealed that relatively small models (millions of parameters) could outperform both general-purpose and domain-specific compressors, achieving a compression ratio as low as 0.49 for OOD audio data. Despite these successes, the transferability of learned representations to unseen modalities remains limited, contrasting with the performance of larger foundation models. This highlights the trade-off between parameter efficiency and cross-modal generalization.

\subsection{An Introduction to Neural Data Compression}
\newpage

\section{Design and Implementation}
\newpage

\section{Project Planning and Scheduling}
\newpage

\section{Expected Outcome}
\subsection{Limitations}
\subsection{Future Enhancements}
\newpage

\section*{References}
\addcontentsline{toc}{section}{\textit{References}}
\newpage

\end{document}
